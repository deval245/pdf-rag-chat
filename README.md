# # ğŸ“„ Chat with Your PDF (LangChain + LangSmith)

Chat with any PDF file using a LangChain-powered RAG pipeline with full tracing via LangSmith.

---

## ğŸš€ Features

- ğŸ“¤ Upload any PDF (policy, resume, contract, article, etc.)
- ğŸ¤– Ask natural language questions about its content
- ğŸ” Retrieves only relevant chunks using FAISS
- ğŸ”— Structured LLM response using LangChain
- ğŸ“Š Full observability via LangSmith
- ğŸ’¡ Document-agnostic and prompt-safe

---

## ğŸ§  Tech Stack

- Python, Streamlit, FastAPI (optional)
- LangChain, FAISS, LangSmith
- OpenAI LLM (or plug your own)
- Unstructured PDF parsing
- Redis/Kafka ready (for future scalability)

---

## ğŸ“ Project Structure

```bash
pdf-rag-chat/
â”œâ”€â”€ chains/               # LangChain logic (RAG)
â”‚   â””â”€â”€ pdf_qa_chain.py
â”œâ”€â”€ utils/                # PDF parsing
â”‚   â””â”€â”€ pdf_loader.py
â”œâ”€â”€ data/                 # Uploaded PDFs (auto-cleared, gitignored)
â”œâ”€â”€ .env                  # API Keys (ignored)
â”œâ”€â”€ main.py               # Streamlit app
â”œâ”€â”€ Makefile              # Quick commands
â”œâ”€â”€ requirements.txt      
â””â”€â”€ README.md             
âš™ï¸ Setup
1. Clone and enter project

git clone https://github.com/yourname/pdf-rag-chat.git

cd pdf-rag-chat

2. Create virtual environment

python3 -m venv venv
source venv/bin/activate

3. Install dependencies
pip3 install -r requirements.txt

make install

4. Setup your .env file
env

OPENAI_API_KEY=sk-xxx
LANGCHAIN_API_KEY=ls-xxx
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=pdf-rag-chat
ğŸ§ª Run the App

make run

or

KMP_DUPLICATE_LIB_OK=TRUE STREAMLIT_WATCHER_TYPE=none streamlit run main.py


ğŸ“Œ LangSmith Tracing
View detailed RAG traces at https://smith.langchain.com

ğŸ’¡ Future Add-ons (PRs Welcome!)
Export answer + source chunks as .txt/.json

Add LangGraph routing for multiple document types

Deploy to Streamlit Cloud with custom subdomain




## ğŸ§  Architecture: Document-Agnostic LLM RAG Pipeline

/Users/devalthakkar/pdf-rag-chat/img.png

This project follows a modular Retrieval-Augmented Generation (RAG) architecture, built for real-time question answering over any PDF (policy, resume, legal, etc.).

### ğŸ§° Core Components

| Layer              | Description |
|-------------------|-------------|
| ğŸ“„ PDF Loader      | Parses document using `UnstructuredPDFLoader` |
| ğŸ§© Chunk Splitter  | Splits into context chunks (RecursiveCharacterTextSplitter) |
| ğŸ§  Embedding Model | OpenAI Embeddings generate semantic vectors |
| ğŸ“¦ Vector DB       | FAISS (in-memory) or **ChromaDB (persistent)** stores chunks |
| ğŸ” Retriever       | Top-k search from VectorDB |
| ğŸ’¬ LLM             | GPT-3.5-turbo generates structured answers |
| ğŸ”— Prompt Template | Controlled, context-only prompt with format rules |
| ğŸ”­ LangSmith Trace | Full traceability of chunk retrieval, LLM steps, and responses |
| ğŸ–¥ï¸ Streamlit UI    | Simple frontend to upload and chat with PDFs |

---

### ğŸ” End-to-End Flow: LLMOps RAG Pipeline

1. **User uploads PDF** â†’ stored locally under `/data`
2. **Chunks created** using tokenizer-aware splitting
3. **Embeddings generated** for chunks via OpenAI
4. **Chunks indexed** into FAISS or Chroma vector DB
5. **User enters question**
6. **Retriever pulls relevant context** from the DB
7. **LLM invoked** with prompt + context â†’ structured answer
8. **Sources + answer shown in UI**
9. **LangSmith logs the entire run** for debugging and observability

ğŸ§‘â€ğŸ’» Author
Deval Thakkar â€“ Sr. Software Engineer | LLM Infra | GenAI Test Architect
ğŸ“ LinkedIn

ğŸ“œ License
MIT License â€” use freely and contribute!


