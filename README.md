ğŸ“„ Chat with Your PDF (LangChain + LangSmith)

Chat with any PDF file using a LangChain-powered RAG pipeline with full tracing via LangSmith.

ğŸš€ Features

ğŸ“¤ Upload any PDF (policy, resume, contract, article, etc.)

ğŸ¤– Ask natural language questions about its content

ğŸ” Retrieves only relevant chunks using FAISS

ğŸ”— Structured LLM response using LangChain

ğŸ“Š Full observability via LangSmith

ğŸ’¡ Document-agnostic and prompt-safe


ğŸ§  Tech Stack

Python, Streamlit

LangChain, FAISS, LangSmith

OpenAI LLM (or Ollama for local use)

Unstructured PDF parsing



ğŸ“ Project Structure

pdf-rag-chat/
â”œâ”€â”€ chains/               # LangChain logic (RAG)
â”‚   â””â”€â”€ pdf_qa_chain.py
â”œâ”€â”€ utils/                # PDF parsing + env loader
â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â””â”€â”€ env_loader.py
â”œâ”€â”€ data/                 # Uploaded PDFs (auto-cleared, gitignored)
â”œâ”€â”€ .env                  # API Keys (ignored)
â”œâ”€â”€ main.py               # Streamlit app
â”œâ”€â”€ Makefile              # Quick commands
â”œâ”€â”€ requirements.txt      
â””â”€â”€ README.md             



âš™ï¸ Setup

1. Clone and enter project

git clone https://github.com/yourname/pdf-rag-chat.git
cd pdf-rag-chat

2. Create virtual environment

python3 -m venv venv
source venv/bin/activate

3. Install dependencies

pip install -r requirements.txt
# or
make install


4. Setup your .env file for local development

OPENAI_API_KEY=sk-...
LANGCHAIN_API_KEY=ls-...
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=pdf-rag-chat

For Streamlit Cloud, keys are entered manually in the UI.




ğŸ” Bring Your Own Key (BYOK) ** most imp

To run this app:

You need your own OpenAI API Key.

Locally, you can use Ollama to avoid key usage.

On Streamlit Cloud, Ollama is NOT supported (no system-level access).

Add your API keys securely via .env file or .streamlit/secrets.toml.

ğŸ“¦ Tip: This app respects your backend toggle and works seamlessly across environments.




ğŸ§ª Run the App

Locally

make run
# or
KMP_DUPLICATE_LIB_OK=TRUE streamlit run main.py

Streamlit Cloud

Push to GitHub and deploy via https://streamlit.io/cloud

Only OpenAI backend is available

Keys prompted from user and not hardcoded



ğŸ” Secure Credential Handling

In local dev, keys loaded from .env file

In Streamlit Cloud, keys prompted via password field

âœ… Keeps your OpenAI and LangSmith keys private



ğŸ“Œ LangSmith Tracing

View full traces and debugging on: https://smith.langchain.com



ğŸ§  Architecture: Document-Agnostic LLM RAG Pipeline

This project follows a modular Retrieval-Augmented Generation (RAG) architecture, built for real-time question answering over any PDF (policy, resume, legal, etc.).



ğŸ§° Core Components

Layer

Description

ğŸ“„ PDF Loader

Parses document using UnstructuredPDFLoader or PyMuPDFLoader



ğŸ§© Chunk Splitter

Splits into context chunks (RecursiveCharacterTextSplitter)



ğŸ§  Embedding Model

OpenAI or HuggingFace sentence embeddings



ğŸ“¦ Vector DB

FAISS (in-memory) or ChromaDB (extensible)

ğŸ” Retriever

Top-k search using LangChain retriever


ğŸ’¬ LLM

GPT-3.5-turbo (OpenAI) or Mistral via Ollama


ğŸ”— Prompt Template

Domain-aware and format-specific prompts


ğŸ”­ LangSmith Trace

Full observability of RAG pipeline


ğŸ–¥ï¸ Streamlit UI

Drag-and-drop PDF + chat interface


ğŸ” End-to-End Flow

User uploads PDF â†’ stored locally in /data

Chunks created with smart overlap for context

Embeddings generated using backend model

Chunks indexed into FAISS

Question entered by user

Top-K context retrieved

LLM invoked with prompt + context

Answer + sources displayed

LangSmith logs the session


ğŸ§‘â€ğŸ’» Author

Deval Thakkar â€“ Sr. Software Engineer | LLM Infra | GenAI Test ArchitectğŸ”— https://www.linkedin.com/in/deval-t-a05486299/


ğŸ“œ License

MIT License â€” use freely and contribute!

